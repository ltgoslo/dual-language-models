#!/bin/bash -e
#SBATCH --job-name=GPT-BERT
#SBATCH --nodes=32
#SBATCH --gpus-per-node=8
#SBATCH --tasks-per-node=8
#SBATCH --cpus-per-task=7
#SBATCH --output="logs/output_%x_%j.txt"
#SBATCH --partition=standard-g
#SBATCH --mem=0
#SBATCH --time=24:00:00
#SBATCH --account=project_465001890


if [ -z $SLURM_JOB_ID ]; then
    mkdir -p logs
    sbatch "$0" "$@"
    exit
fi

# distributed setup
export MASTER_ADDR=$(scontrol show hostnames "$SLURM_JOB_NODELIST" | head -n 1)
export MASTER_PORT=11643
export WORLD_SIZE=$SLURM_NTASKS

module use /appl/local/csc/modulefiles/
module load pytorch/2.7

c=fe
MYMASKS="0x${c}000000000000,0x${c}00000000000000,0x${c}0000,0x${c}000000,0x${c},0x${c}00,0x${c}00000000,0x${c}0000000000"

srun --label ./launch.sh $@